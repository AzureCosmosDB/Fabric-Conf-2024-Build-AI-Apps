{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pattern Application - Simple Implementation\n",
    "\n",
    "In this sample, we'll demonstrate how to build a RAG Pattern application using a subset of the Movie Lens dataset. This sample will leverage the SDK for Azure Cosmos DB for NoSQL to perform vector search and cache the results. And Azure OpenAI to generate embeddings and LLM completions.\n",
    "\n",
    "There are two implementations in this project. One using LangChain and this simple implementation. The simple implementation connects directly to Azure Cosmos DB for NoSQL to perform vector search, and cache responses. It also connects directly to Azure OpenAI to generate embeddings and completions. This version requires a user to define and build the LLM payloads for LLM generation and also define the RAG Pattern request pipeline. Cache must be manually consulted in the pipeline and responses must also be manually cached.\n",
    "\n",
    "The vector search will be done using Azure Cosmos DB for SQL's vector similarity search functionality to do vector search over the vectorized movie data as well as the conversation history which is also used as a cache.\n",
    "\n",
    "At the end we will create a simple UX using Gradio to allow users to type in questions and display responses generated by a GPT model or served from the cache. The resopnses will also display an elapsed time so you can see the impact caching has on performance versus generating a response.\n",
    "\n",
    "**Important Note**\n",
    "This sample requires you to have the Azure Cosmos DB for NoSQL account setup with the Movies data uploaded to a container with vector indexing setup. Additionally, you also need to setup another container with vector indexing for setting up cache. To know more about how to setup vector search enabled containers, please refer to [this notebook](https://aka.ms/vector-search-nosql-nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries <a class=\"anchor\" id=\"preliminaries\"></a>\n",
    "First, let's start by installing the packages that we'll need later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install json\n",
    "! pip install python-dotenv\n",
    "\n",
    "! pip install openai\n",
    "\n",
    "! pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import time\n",
    "import json\n",
    "import uuid\n",
    "from dotenv import dotenv_values\n",
    "from openai import AzureOpenAI\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "#Cosmos DB imports\n",
    "from azure.cosmos import CosmosClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please use the example.env as a template to provide the necessary keys and endpoints in your own .env file.\n",
    "Make sure to modify the env_name accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "# specify the name of the .env file name \n",
    "env_name = \"fabcondemo.env\" # following example.env template change to your own .env file name\n",
    "config = dotenv_values(env_name)\n",
    "\n",
    "cosmos_conn = config['cosmos_connection_string']\n",
    "cosmos_key = config['cosmos_key']\n",
    "cosmos_database = config['cosmos_database_name']\n",
    "cosmos_collection = config['cosmos_collection_name']\n",
    "cosmos_vector_property = config['cosmos_vector_property_name']\n",
    "comsos_cache_db = config['cosmos_cache_database_name']\n",
    "cosmos_cache = config['cosmos_cache_collection_name']\n",
    "# Create the Azure Cosmos DB for NoSQL client\n",
    "cosmos_client = CosmosClient(url=cosmos_conn, credential=cosmos_key)\n",
    "\n",
    "openai_endpoint = config['openai_endpoint']\n",
    "openai_key = config['openai_key']\n",
    "openai_api_version = config['openai_api_version']\n",
    "openai_embeddings_deployment = config['openai_embeddings_deployment']\n",
    "openai_embeddings_dimensions = int(config['openai_embeddings_dimensions'])\n",
    "openai_completions_deployment = config['openai_completions_deployment']\n",
    "# Create the OpenAI client\n",
    "openai_client = AzureOpenAI(azure_endpoint=openai_endpoint, api_key=openai_key, api_version=openai_api_version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database and collections\n",
    "\n",
    "Please make sure that you have the movies and the cache containers setup already. To know more about how to setup vector search enabled containers, please refer to [this notebook](https://aka.ms/vector-search-nosql-nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get databases and containers to work with\n",
    "\n",
    "db = cosmos_client.get_database_client(cosmos_database)\n",
    "movies_container = db.get_container_client(cosmos_collection)\n",
    "cache_container = db.get_container_client(cosmos_cache)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate embeddings from Azure OpenAI\n",
    "\n",
    "This is used to vectorize the user input for the vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate openai embeddings\n",
    "def generate_embeddings(text):    \n",
    "    '''\n",
    "    Generate embeddings from string of text.\n",
    "    This will be used to vectorize data and user input for interactions with Azure OpenAI.\n",
    "    '''\n",
    "    print(\"Generating embeddings for: \", text, \" with model: \", openai_embeddings_deployment)\n",
    "    response = openai_client.embeddings.create(input=text, model=openai_embeddings_deployment)\n",
    "    embeddings =response.model_dump()\n",
    "    time.sleep(0.5) \n",
    "    return embeddings['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Search in Azure Cosmos DB for MongoDB\n",
    "\n",
    "This defines a function for performing a vector search over the movies data and chat cache collections. Function takes a collection reference, array of vector embeddings, and optional similarity score to filter for top matches and number of results to return to filter further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a vector search on the Cosmos DB container\n",
    "def vector_search(container, vectors, similarity_score=0.02, num_results=3):\n",
    "    \n",
    "    # Execute the query\n",
    "    results = list(container.query_items(\n",
    "        query='SELECT TOP @num_results c.title,c.overview,c.completion, VectorDistance(c.embeddings,@embedding, false, {\"distanceFunction\": \"cosine\"}) as SimilarityScore FROM c WHERE VectorDistance(c.embeddings,@embedding, true, {\"distanceFunction\": \"cosine\"}) > @similarity_score',\n",
    "        parameters=[\n",
    "            {\"name\": \"@embedding\", \"value\": vectors},\n",
    "            {\"name\": \"@num_results\", \"value\": num_results},\n",
    "            {\"name\": \"@similarity_score\", \"value\": similarity_score}\n",
    "        ],\n",
    "        enable_cross_partition_query=True)\n",
    "        )\n",
    "\n",
    "    # Extract the necessary information from the results\n",
    "    formatted_results = []\n",
    "    for result in results:\n",
    "        formatted_result = {\n",
    "            'similarityScore': result['SimilarityScore'],\n",
    "            'document': result\n",
    "        }\n",
    "        formatted_results.append(formatted_result)\n",
    "    \n",
    "    print(formatted_results)\n",
    "\n",
    "    return formatted_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get recent chat history\n",
    "\n",
    "This function provides conversational context to the LLM, allowing it to better have a conversation with the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab chat history to as part of the payload to GPT model for completion.\n",
    "def get_chat_history(history_container, completions=3):\n",
    "    # Query Cosmos DB to retrieve chat history\n",
    "    query = f\"SELECT TOP {completions} c.prompt, c.completion FROM c ORDER BY c._ts DESC\"\n",
    "    items = list(history_container.query_items(query=query, enable_cross_partition_query=True))\n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Completion Function\n",
    "\n",
    "This function assembles all of the required data as a payload to send to a GPT model to generate a completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion(cache_container, movies_container, user_input):\n",
    "\n",
    "    # Generate embeddings from the user input\n",
    "    print(\"1\\n\")\n",
    "    user_embeddings = generate_embeddings(user_input)\n",
    "    print(\"10\\n\")\n",
    "    # Query the chat history cache first to see if this question has been asked before\n",
    "    cache_results = vector_search(container = cache_container, vectors = user_embeddings, similarity_score=0.99, num_results=1)\n",
    "\n",
    "    if len(cache_results) > 0:\n",
    "        print(\"11\\n Cached Result\\n\")\n",
    "        return cache_results[0]['document']['completion']\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        #perform vector search on the movie collection\n",
    "        print(\"2\\n New result\\n\")\n",
    "        search_results = vector_search(movies_container, user_embeddings)\n",
    "\n",
    "        print(\"Getting Chat History\\n\")\n",
    "        #chat history\n",
    "        chat_history = get_chat_history(cache_container, 3)\n",
    "\n",
    "        #generate the completion\n",
    "        print(\"Generating completions \\n\")\n",
    "        completions_results = generate_completion(user_input, search_results, chat_history)\n",
    "\n",
    "        print(\"Caching response \\n\")\n",
    "        #cache the response\n",
    "        cache_response(cache_container, user_input, user_embeddings, completions_results)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        # Return the generated LLM completion\n",
    "        return completions_results['choices'][0]['message']['content'] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache Generated Responses\n",
    "\n",
    "Save the user prompts and generated completions in a conversation. Used to answer the same questions from other users. This is cheaper and faster than generating results each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_response(cache_container, user_prompt, prompt_vectors, response):\n",
    "    container = cache_container\n",
    "\n",
    "    # Create a dictionary representing the chat document\n",
    "    chat_document = {\n",
    "        'id': str(uuid.uuid4()),  # Generate a unique ID for the document\n",
    "        'prompt': user_prompt,\n",
    "        'completion': response['choices'][0]['message']['content'],\n",
    "        'completionTokens': str(response['usage']['completion_tokens']),\n",
    "        'promptTokens': str(response['usage']['prompt_tokens']),\n",
    "        'totalTokens': str(response['usage']['total_tokens']),\n",
    "        'model': response['model'],\n",
    "        'embeddings': prompt_vectors\n",
    "    }\n",
    "\n",
    "    # Insert the chat document into the Cosmos DB container\n",
    "    container.create_item(body=chat_document)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Pipeline function\n",
    "\n",
    "This function defines the pipeline for our RAG Pattern application. When user submits a question, the cache is consulted first for an exact match. If no match then a vector search is made, chat history gathered, the LLM generates a response, which is then cached before returning to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion(cache_container, movies_container, user_input):\n",
    "\n",
    "    # Generate embeddings from the user input\n",
    "    print(\"1\\n\")\n",
    "    user_embeddings = generate_embeddings(user_input)\n",
    "    print(\"10\\n\")\n",
    "    # Query the chat history cache first to see if this question has been asked before\n",
    "    cache_results = vector_search(container = cache_container, vectors = user_embeddings, similarity_score=0.99, num_results=1)\n",
    "\n",
    "    if len(cache_results) > 0:\n",
    "        print(\"11\\n Cached Result\\n\")\n",
    "        return cache_results[0]['document']['completion']\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        #perform vector search on the movie collection\n",
    "        print(\"2\\n New result\\n\")\n",
    "        search_results = vector_search(movies_container, user_embeddings)\n",
    "\n",
    "        print(\"Getting Chat History\\n\")\n",
    "        #chat history\n",
    "        chat_history = get_chat_history(cache_container, 3)\n",
    "\n",
    "        #generate the completion\n",
    "        print(\"Generating completions \\n\")\n",
    "        completions_results = generate_completion(user_input, search_results, chat_history)\n",
    "\n",
    "        print(\"Caching response \\n\")\n",
    "        #cache the response\n",
    "        cache_response(cache_container, user_input, user_embeddings, completions_results)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        # Return the generated LLM completion\n",
    "        return completions_results['choices'][0]['message']['content'] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a simple UX in Gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox(label=\"Ask me anything about movies!\")\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def user(user_message, chat_history):\n",
    "\n",
    "        # Create a timer to measure the time it takes to complete the request\n",
    "        start_time = time.time()\n",
    "        \n",
    "        \n",
    "        print(\"5\\n\")\n",
    "        # Get LLM completion\n",
    "        response_payload = chat_completion(cache_container, movies_container, user_message)\n",
    "\n",
    "        # Stop the timer\n",
    "        end_time = time.time()\n",
    "\n",
    "        elapsed_time = round((end_time - start_time) * 1000, 2)\n",
    "\n",
    "        response = response_payload\n",
    "        \n",
    "        # Append user message and response to chat history\n",
    "        chat_history.append([user_message, response_payload + f\"\\n (Time: {elapsed_time}ms)\"])\n",
    "        \n",
    "        return gr.update(value=\"\"), chat_history\n",
    "    \n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False)\n",
    "    \n",
    "    clear.click(lambda: None, None, chatbot, queue=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch the gradio interface\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# be sure to run this cell to close or restart the gradio demo\n",
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
